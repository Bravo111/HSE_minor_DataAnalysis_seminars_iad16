{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Майнор \"Интеллектуальный анализ данных\", Введение в Анализ данных (ВШЭ). Семинар 5: библиотека sklearn и метод ближайшего соседа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn: machine learning in Python \n",
    "В библиотеке __sklearn__ реализованы все основные методы машинного обучения. Для них предусмотрены удобные унифированные интерфейсы (и их копируют другие библиотеки) и достаточно эффективные алгоритмы (хотя, как правило, sklearn используют для выбора алгоритма и модели, а затем их уже пишут на низкоуровневых языках с целью повышения скорости работы). Кроме того, sklearn предоставляет некоторые дополнительные средства, в частности, для предобработки данных и для выбора модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/\n",
    "\n",
    "Обратите внимание на многообразие документации: быстрый старт, полная документация (Usrr Guide), tutuorials (для решения стандартных задач), API (методы, классы, параметры) и красивый flowchart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Любите документацию sklearn, читайте ее, наслаждайтесь ей! Подсматривайте в ней интерфейсы методов и делайте новые открытия каждый день :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры датасетов"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Documentation -> User Guide -> Dataset Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DESCR __class__ __cmp__ __contains__ __delattr__ __delitem__ __dict__ __doc__ __eq__ __format__ __ge__ __getattribute__ __getitem__ __gt__ __hash__ __init__ __iter__ __le__ __len__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __setitem__ __sizeof__ __str__ __subclasshook__ __weakref__ clear copy data feature_names fromkeys get has_key items iteritems iterkeys itervalues keys pop popitem setdefault target update values viewitems viewkeys viewvalues'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(dir(boston))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data | feature_names | DESCR | target'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" | \".join(boston.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506L, 13L)\n"
     ]
    }
   ],
   "source": [
    "print boston.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В sklearn всегда (как и в теории) строки --- это объекты, столбцы --- это признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.32000000e-03   0.00000000e+00   4.60000000e-01   0.00000000e+00\n",
      "   3.85000000e-01   3.56100000e+00   2.90000000e+00   1.12960000e+00\n",
      "   1.00000000e+00   1.87000000e+02   1.26000000e+01   3.20000000e-01\n",
      "   1.73000000e+00]\n",
      "[  88.9762  100.       27.74      1.        0.871     8.78    100.\n",
      "   12.1265   24.      711.       22.      396.9      37.97  ]\n",
      "[  3.59376071e+00   1.13636364e+01   1.11367787e+01   6.91699605e-02\n",
      "   5.54695059e-01   6.28463439e+00   6.85749012e+01   3.79504269e+00\n",
      "   9.54940711e+00   4.08237154e+02   1.84555336e+01   3.56674032e+02\n",
      "   1.26530632e+01]\n"
     ]
    }
   ],
   "source": [
    "print boston.data.min(axis=0)\n",
    "print boston.data.max(axis=0)\n",
    "print boston.data.mean(axis=0)\n",
    "# В pandas это можно было сделать одной строчкой кода, и еще мы бы знали названия признаков..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# но названия признаков можно посмотреть в описании\n",
    "print boston.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие датасеты в sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__all__ | __builtins__ | __doc__ | __file__ | __name__ | __package__ | __path__ | _svmlight_format | base | california_housing | clear_data_home | covtype | dump_svmlight_file | fetch_20newsgroups | fetch_20newsgroups_vectorized | fetch_california_housing | fetch_covtype | fetch_lfw_pairs | fetch_lfw_people | fetch_mldata | fetch_olivetti_faces | fetch_species_distributions | get_data_home | lfw | load_boston | load_diabetes | load_digits | load_files | load_iris | load_lfw_pairs | load_lfw_people | load_linnerud | load_mlcomp | load_sample_image | load_sample_images | load_svmlight_file | load_svmlight_files | make_biclusters | make_blobs | make_checkerboard | make_circles | make_classification | make_friedman1 | make_friedman2 | make_friedman3 | make_gaussian_quantiles | make_hastie_10_2 | make_low_rank_matrix | make_moons | make_multilabel_classification | make_regression | make_s_curve | make_sparse_coded_signal | make_sparse_spd_matrix | make_sparse_uncorrelated | make_spd_matrix | make_swiss_roll | mlcomp | mldata | mldata_filename | olivetti_faces | samples_generator | species_distributions | svmlight_format | twenty_newsgroups'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "\" | \".join(dir(sklearn.datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение с учителем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого метода в sklearn реализован отдельный класс, например, sklearn.neighbors.KNeighborsClassifier. \n",
    "При вызове его конструктора необходимо указать параметры метода или алгоритма обучения (но для них, как правило, есть значения по умолчанию). \n",
    "\n",
    "Для того, чтобы обучить модель по данным, нужно воспользоваться методом __fit(X, y)__, где X --- матрица объекты-признаки, y - значение целевой переменной (дискретная для классификации и непрерывная для регрессии). Обязательное условие: X.shape[0] == y.shape[0] (в i-й строке y находится правильный ответ для i-го объекта в X). В результате в модели настроятся параметры, необходимые для предсказаний).\n",
    "\n",
    "Чтобы сделать предсказание на новых данных, нужно воспользоваться методом __predict(X)__, X --- матрица объекты-признаки. Метод вернет вектор y.\n",
    "\n",
    "Часто класс также предоставляет метод __predict_proba(X)__, который возвращает ответы и степень уверенности классификатора или регрессора в них (вероятность того, что объект принадлежит какому-то классу). Более конкретно, для классификации метод возвращает матрицу размера число объектов x число классов, состоящую из таких вероятностей.\n",
    "\n",
    "Кроме того, у каждого класса есть методы __get_params()__, который вернет словарь параметров (тех, которые указываются в конструкторе) и __set_params(**params)__, который может менять значения параметров."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Documentation -> Tutorials -> Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение без учителя (кластеризация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для кластеризации интерфейс похожий, но из-за отсутствия целевой переменной в fit нужно подавать только матрицу X. \n",
    "\n",
    "Также есть метод fit_predict(X), который находит кластеры и возвращает вектор принадлежности объектов кластерам."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Documentation -> Tutorials -> Unsupervised Learning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Documentation -> Tutorials -> An introduction to machine learning with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохранение построенной модели\n",
    "import pickle\n",
    "with open(\"file.txt\", \"w\") as f_out:\n",
    "    pickle.dump(clf, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе с реальными данными можно попасть в ситацию, когда вы запускаете обучение модели и не видите прогресс в обучении (cell долго работает, и вы не знаете, когда он закончит работать). Поэтому, при возникновении подобной ситуации, рекомендуется запустить обучение несколько раз на совсем маленьких выборках, чтобы понять, как время обучения зависит от размера входных данных (например, построить график), и уже оценив его для всей выборки, решать, сможете ли вы так долго ждать обучения модели. \n",
    "\n",
    "Такая же ситуация может возникнуть и в методом predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы будем изучать, как все эти методы работают. И начнем с наиболее простой группы методов, на примере которой еще раз вспомним основные аспекты решения задач машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрические классификаторы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В машинном обучении распространено понятие __эвристики__ --- некоторой идеи, которая кажется правдоподобной (хотя не имеет четкого обоснования) и на которой может основываться метод или улучшение какого-то метода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Напоминание: обучение модели\n",
    "Задача машинного обучения выглядит так: дана обучающая выборка в виде матрицы объекты-признаки и задан какой-то вопрос относительно объектов; нужно научиться отвечать на это вопрос для новых объектов. Для этого придумывают модель, имеющую некоторые параметры, которые настраивают с помощью оптимизации какого-то критерия. Кроме того, у модели могут быть гиперпараметры, не настраиваемые в процессе обучения, которые нужно задавать самостоятельно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Напоминание: шумовой объект\n",
    "В машинном обучении приходится работать с совершенно неидеальными данными, в которых увидеть закономерность можно только по достаточно большой выборке. В этой выборке могут встречаться объкты, которые противоречат основной тенденции остальных объектов (например, деревня по многим показателям похожа на город). Такие объекты называют _шумовыми_, или _выбросами_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Напоминание: переобучение\n",
    "Переобучением называется явление, когда алгоритм выдает маленькую ошибку на обучающей выборке, но большую ошибку на новых данных. В этом случае алгоритм настраивается на шум в данных и перестает улавливать закономерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Напоминание: задача классификации\n",
    "\n",
    "Дана обучающая выборка (матрица объекты-признаки), выделен один признак (целевой), у которого множество значений конечно (например, цифры, категории, бинарный ответ да/нет). Этот признак мы и хотим предсказывать для новых объектов. Классификация может быть бинарной и многоклассовой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Простая эвристика__: давайте выдавать такой же ответ, какой является правильным для объекта обучающей выборки, наиболее похожего на данный (это будет наша _модель ответа_). \n",
    "Такой классификатор называется __Метод ближайшего соседа__.\n",
    "\n",
    "В его основе лежит _предположение_ о том, что объекты одного класса лежат близко друг к другу в пространстве признаков (_гипотеза компактности_).\n",
    "\n",
    "Стоит обратить внимание, что это чуть ли не единственный метод машинного обучения, в котором процедура _обучения_ состоит в _запоминании_ выборки (не нужно ничего оптимизировать)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, легко понять, что для двумерного случая классификация всей плоскости задается разбиением ее на многоугольники, ограниченные серединными перпендикулярами к отрезкам, попарно соединяющим объекты обучающей выборки.\n",
    "Это показано на левой картинке (здесь выборка состоит из 8 объектов, целевой признак --- цвет точки: синий или зеленый, каждый объект представлен двумя признаками-координатами на вещественной плоскости)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://shapeofdata.files.wordpress.com/2013/03/knn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь кроется и вся опасность МБС --- его склонность к переобучению: для любого шумового объекта (например, правой нижней синей точки) будет создаваться отдельная область, то есть алгоритм крайне чувствителен к шуму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте немного усложним классификатор: вместо одного соседа будем рассматривать k наиболее похожих на данный объект, и выбирать тот класс, который наиболее часто встречается среди k соседей (новая _модель_). Получится классификатор, более устойчивый к переобучению. Этот метод называется __метод k ближайших соседей, или KNN__. Например, на правой картинке показана разделяющая поверхность при k=3, и там уже синий объект признается шумовым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, число соседей k является _гиперпараметром_ модели, который нужно задавать перед обучением. Для этого, как правило, используют кросс-валидацию (о ней чуть-чуть упоминали на лекциях, и мы позже разберем на семинарах)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обсуждение kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ряда задач (например, для распознавания цифр или классификации текстов, представленных в виде <<мешка слов>>), kNN дает хороший результат. Ключевой вопрос состоит в __выборе метрики для сравнения объектов__. Для вещественных объектов $x = (x_1, \\dots, x_d)$ и $y=(y_1, \\dots, y_d)$ используют:\n",
    "1. расстояние Минковского с показателем $p$: $\\rho(x, y) = \\biggl(\\sum_{i=1}^d (x_i - y_i)^p\\biggr)^{\\frac 1 p}$; при $p=2$ это привычное _евклидово расстояние_.\n",
    "1. косинусную метрику:  $\\rho(x, y) = \\frac {<x, y>} {\\sqrt{<x, x><y, y>}}$, $<. ,.>$ --- скаалярное произведение: $<x, y> = \\sum_{i=1}^d x_i y_i$. Это расстояние равно косинусу угла между векторами $x$ и $y$ в евклидовом пространстве; хорошо работает для текстов.\n",
    "\n",
    "Для бинарных векторов (каждый компонент есть 0 или 1):\n",
    "1. расстояние Хэмминга --- число несовпадающих разрядов. \n",
    "1. расстояние Жаккарда --- число единиц в поэлементной конъюнкции, деленное на число элемент в поэлементной дизъюнкции.\n",
    "\n",
    "Последнее есть интерпретация расстояния Жаккарда для множеств A и B:\n",
    "1. $\\rho(A, B) = \\frac {|A \\cap B|}{|A \\cup B|}$\n",
    "\n",
    "(Подробнее о метриках здесь: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)\n",
    "\n",
    "Если в задаче известны признаки разной природы, можно составлять комбинированную метрику. \n",
    "\n",
    "Более того, можно вводить веса признаков $w = (w_1, \\dots, w_d)$ и модифицировать метрики, например:\n",
    "$\\rho(x, y) = \\sqrt{\\sum_{i=1}^d w_i(x_i - y_i)^2}$.\n",
    "\n",
    "Последний прием важен на _ненормированных_ выборках (если в сумме одно слагаемое имеет порядок 1000, а другое 0.1, то второе не будет иметь влияния на величину расстояния). Тогда в качестве весов можно использовать обратные к средним или к максимальным значениям признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса можно применять и к объектам, тогда объекты, находящиеся на расстоянии, более близком к новой точке, будут иметь большее влияние."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN в sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерфейс описан в [документации](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# импортируем классификатор\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# загружаем данные --- изображения цифр\n",
    "from sklearn.datasets import load_digits\n",
    "data = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.images\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797L, 8L, 8L)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], -1) # вытягиваем квадратное изображение в вектор, чтобы получить матрицу объекты-признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы избежать переобучения (а также чтобы подобрать k), нужно разделить выборку на train и test. Для этого объекты нужно случайно перемешать и разделить в какой-то пропорции:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "google-поиск sklearn shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797L, 64L), (1797L,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape # проверяем, что все хорошо перемешалось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 8, 6, 5, 2, 5, 6, 5, 6])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10] # теперь в случайном порядке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = X[:700, :], y[:700]\n",
    "X_val, y_val = X[700:1300, :], y[700:1300] #validation\n",
    "X_test, y_test = X[1300:, :], y[1300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обучаем классификатор и делаем предсказания\n",
    "clf.fit(X_train, y_train)\n",
    "y_predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.955734406439\n"
     ]
    }
   ],
   "source": [
    "# Вычисляем простейшую метрику качества алгоритма --- долю правильных ответов\n",
    "print \"Accuracy is\", np.mean(y_test==y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учитывая, что у нас 10 классов, и вероятность случайно вытащить два одинаковых маленькая, точность 0.956 --- очень хороший результат!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 ; accuracy = 0.958333333333\n",
      "k = 2 ; accuracy = 0.956666666667\n",
      "k = 3 ; accuracy = 0.956666666667\n",
      "k = 4 ; accuracy = 0.956666666667\n",
      "k = 5 ; accuracy = 0.958333333333\n",
      "k = 6 ; accuracy = 0.958333333333\n",
      "k = 7 ; accuracy = 0.963333333333\n",
      "k = 8 ; accuracy = 0.965\n",
      "k = 9 ; accuracy = 0.965\n",
      "k = 10 ; accuracy = 0.966666666667\n",
      "k = 11 ; accuracy = 0.958333333333\n",
      "k = 12 ; accuracy = 0.961666666667\n",
      "k = 13 ; accuracy = 0.96\n",
      "k = 14 ; accuracy = 0.956666666667\n",
      "k = 15 ; accuracy = 0.951666666667\n",
      "k = 16 ; accuracy = 0.946666666667\n",
      "k = 17 ; accuracy = 0.94\n",
      "k = 18 ; accuracy = 0.94\n",
      "k = 19 ; accuracy = 0.94\n"
     ]
    }
   ],
   "source": [
    "# Подбор k на валидационной выборке:\n",
    "for k in range(1, 20):\n",
    "    y_predicted = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train).predict(X_val)\n",
    "    print \"k =\", k, \"; accuracy =\", np.mean(y_predicted==y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучший результат при k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним точность (accuracy) на трех датасетах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.987142857143\n",
      "Accuracy: 0.966666666667\n",
      "Accuracy: 0.941649899396\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=10)\n",
    "clf.fit(X_train, y_train)\n",
    "for X_data, y_data in zip([X_train, X_val, X_test], [y_train, y_val, y_test]):\n",
    "    y_predicted = clf.predict(X_data)\n",
    "    print \"Accuracy:\", np.mean(y_predicted==y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на обучающей выборке самое лучшее, но оно обманчиво, ведь алгоритм уже знает эти объекты (переобучение).\n",
    "На валидационной выборке мы тоже использовали ответы --- уже для подбора гиперпараметра k, так что этот показатель тоже не совсем честный. \n",
    "И действительно, качество на тестовой выборке (ответы для которой мы нигде не подсматривали) оказалось хуже, чем на валидационной выборке.\n",
    "\n",
    "_Вывод_: оценивать качество алгоритма нужно на отложенной выборке, которая не используется нигде в обучении."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
